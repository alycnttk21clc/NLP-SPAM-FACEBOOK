{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "409c703a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "!pip install -q gradio scikit-learn joblib matplotlib seaborn xgboost imbalanced-learn\n\nimport re, unicodedata\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, precision_recall_curve, f1_score\nfrom imblearn.over_sampling import RandomOverSampler\n\nCSV_PATH = \"/content/facebook_captions_comments_spam_dataset (1).csv\"  # s\u1eeda n\u1ebfu kh\u00e1c\n\nencodings_to_try = [\"utf-8-sig\", \"utf-8\", \"cp1258\", \"latin-1\"]\ndf = None\nfor enc in encodings_to_try:\n    try:\n        df = pd.read_csv(CSV_PATH, encoding=enc, low_memory=False)\n        print(\"\u0110\u00e3 \u0111\u1ecdc file v\u1edbi encoding:\", enc)\n        break\n    except Exception as e:\n        print(\"L\u1ed7i v\u1edbi\", enc, \":\", e)\n\nif df is None:\n    raise ValueError(\"Kh\u00f4ng \u0111\u1ecdc \u0111\u01b0\u1ee3c file CSV v\u1edbi encodings th\u1eed.\")\n\nprint(\"Columns:\", df.columns.tolist())\ndisplay(df.head(5))\n\ndef remove_accents(txt):\n    if not isinstance(txt, str): return txt\n    nkfd = unicodedata.normalize(\"NFKD\", txt)\n    return \"\".join([c for c in nkfd if not unicodedata.combining(c)])\n\ndef clean_text(s, remove_accent=False):\n    if not isinstance(s, str): return \"\"\n    s = s.strip()\n    s = re.sub(r\"http\\S+|www\\.\\S+\", \" <URL> \", s)\n    s = re.sub(r\"\\+?\\d[\\d\\-\\s]{7,}\\d\", \" <PHONE> \", s)\n    s = re.sub(r\"[^\\w\u00c0-?<>\\s]\", \" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    s = s.lower()\n    if remove_accent:\n        s = remove_accents(s)\n    return s\n\ncaption_col = None\nfor c in [\"caption\",\"Caption\"]:\n    if c in df.columns:\n        caption_col = c\n        break\n\ncomment_col = None\nfor c in [\"comment\",\"coment\",\"comments\",\"Comment\",\"Coment\"]:\n    if c in df.columns:\n        comment_col = c\n        break\n\nif comment_col is None:\n    raise KeyError(\"Kh\u00f4ng t\u00ecm th\u1ea5y c\u1ed9t comment/coment trong file.\")\n\nif caption_col:\n    df[\"text\"] = df[caption_col].fillna(\"\").astype(str) + \" \" + df[comment_col].fillna(\"\").astype(str)\nelse:\n    df[\"text\"] = df[comment_col].fillna(\"\").astype(str)\n\ndf[\"text\"] = df[\"text\"].apply(lambda z: clean_text(z, remove_accent=False))\n\nif \"label\" not in df.columns:\n    raise KeyError(\"Kh\u00f4ng th\u1ea5y c\u1ed9t 'label' trong file.\")\n\nif df['label'].dtype == object:\n    df['label_num'] = df['label'].map({'ham':0,'spam':1})\nelse:\n    df['label_num'] = pd.to_numeric(df['label'], errors='coerce')\ndf['label_num'] = df['label_num'].fillna(0).astype(int)\n\nprint(\"Sample text after clean:\")\ndisplay(df[[\"text\",\"label\",\"label_num\"]].head(5))\n\ndef extract_numeric_features_from_text(raw):\n    s = \"\" if pd.isna(raw) else str(raw)\n    lower = s.lower()\n    has_url = int(bool(re.search(r'<url>', lower))) or int(bool(re.search(r'http[s]?://|www\\.', lower)))\n    has_phone = int(bool(re.search(r'<phone>', lower))) or int(bool(re.search(r'\\d{9,}', lower)))\n    num_digits = sum(c.isdigit() for c in s)\n    num_words = len(re.findall(r\"[\\w\u00c0-?]+\", s))\n    num_exclaim = s.count('!')\n    num_question = s.count('?')\n    num_at = s.count('@')\n    num_hash = s.count('#')\n    upper_ratio = (sum(1 for c in s if c.isupper()) / (len(s) if len(s)>0 else 1))\n    kw_pattern = r'gi\u1ea3m|sale|khuy\u1ebfn m\u00e3i|mi\u1ec5n ph\u00ed|voucher|mua ngay|click|link|inbox|zalo|b\u00e1n|gi\u00e1 r\u1ebb|t\u1eb7ng'\n    has_discount_kw = int(bool(re.search(kw_pattern, lower)))\n    return pd.Series({\n        \"has_url\": has_url,\n        \"has_phone\": has_phone,\n        \"num_digits\": num_digits,\n        \"num_words\": num_words,\n        \"num_exclaim\": num_exclaim,\n        \"num_question\": num_question,\n        \"num_at\": num_at,\n        \"num_hash\": num_hash,\n        \"upper_ratio\": upper_ratio,\n        \"has_discount_kw\": has_discount_kw\n    })\n\nnum_feats = df['text'].apply(extract_numeric_features_from_text)\ndf = pd.concat([df.reset_index(drop=True), num_feats.reset_index(drop=True)], axis=1)\n\nnumeric_cols = [\"has_url\",\"has_phone\",\"num_digits\",\"num_words\",\"num_exclaim\",\"num_question\",\"num_at\",\"num_hash\",\"upper_ratio\",\"has_discount_kw\"]\nprint(\"Feature cols:\", numeric_cols)\ndisplay(df[numeric_cols+[\"label_num\"]].head(5))\n\nX = df[[\"text\"] + numeric_cols].copy()\ny = df[\"label_num\"].copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nprint(\"Class distribution train:\\n\", y_train.value_counts())\nprint(\"Class distribution test:\\n\", y_test.value_counts())\n\nros = RandomOverSampler(random_state=42)\nX_train_res, y_train_res = ros.fit_resample(X_train, y_train)\nX_train_res = pd.DataFrame(X_train_res, columns=X_train.columns)\n\nprint(\"After oversample train dist:\", np.bincount(y_train_res))\n\ndef vi_tokenizer(text):\n    return re.findall(r\"[\\w\u00c0-?]+\", text)\n\ntf_word = TfidfVectorizer(tokenizer=vi_tokenizer, lowercase=False, max_df=0.95, min_df=2, ngram_range=(1,3), sublinear_tf=True, max_features=20000)\ntf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), min_df=3)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"tf_word\", tf_word, \"text\"),\n        (\"tf_char\", tf_char, \"text\"),\n        (\"num\", StandardScaler(), numeric_cols)\n    ],\n    remainder='drop'\n)\n\npreprocessor.fit(X_train_res)\nprint(\"Vocab sizes:\", len(preprocessor.named_transformers_['tf_word'].get_feature_names_out()),\n      len(preprocessor.named_transformers_['tf_char'].get_feature_names_out()))\n\npipeline = Pipeline([\n    (\"pre\", preprocessor),\n    (\"clf\", LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=\"balanced\"))\n])\n\nparam_grid = {\n    \"pre__tf_word__ngram_range\": [(1,2),(1,3)],\n    \"pre__tf_word__max_df\": [0.9, 0.95],\n    \"clf__C\": [0.1, 1, 5]\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid = GridSearchCV(pipeline, param_grid, cv=cv, scoring=\"f1\", n_jobs=-1, verbose=1)\n\ngrid.fit(X_train_res, y_train_res)\n\nbest_model = grid.best_estimator_\nprint(\"Best params:\", grid.best_params_)\nprint(\"Best CV f1:\", grid.best_score_)\n\ny_pred = best_model.predict(X_test)\ny_prob = best_model.predict_proba(X_test)[:,1] if hasattr(best_model, \"predict_proba\") else None\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"F1:\", f1_score(y_test, y_pred))\nprint(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=[\"ham\",\"spam\"]))\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"ham\",\"spam\"], yticklabels=[\"ham\",\"spam\"])\nplt.title(\"Confusion matrix\")\nplt.show()\n\nif y_prob is not None:\n    fpr, tpr, _ = roc_curve(y_test, y_prob)\n    print(\"AUC:\", auc(fpr,tpr))\n    plt.figure(figsize=(5,5)); plt.plot(fpr,tpr,label=f\"AUC={auc(fpr,tpr):.4f}\"); plt.plot([0,1],[0,1],'--',color='gray'); plt.legend(); plt.show()\n    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n    plt.figure(figsize=(5,5)); plt.plot(rec,prec); plt.title(\"Precision-Recall\"); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.show()\n\nprint(\"Cross-val F1 (5-fold) on full data (slow):\")\nprint(cross_val_score(best_model, X, y, cv=5, scoring=\"f1\", n_jobs=-1))\n\njoblib.dump(best_model, \"/content/best_spam_improved.joblib\")\nprint(\"Saved model to /content/best_spam_improved.joblib\")\n\nfrom gradio import Blocks, Row, Textbox, Button, Markdown\n\nbest_pipe = joblib.load(\"/content/best_spam_improved.joblib\")\n\ndef make_features_row_from_caption_comment(caption, comment):\n    text = (\"\" if caption is None else str(caption)) + \" \" + (\"\" if comment is None else str(comment))\n    text = clean_text(text, remove_accent=False)\n    return pd.DataFrame([{\n        \"text\": text,\n        **extract_numeric_features_from_text(text)\n    }])\n\ndef explain_top_tokens(text, top_k=8):\n    try:\n        text_clean = clean_text(text, remove_accent=False)\n        tf_word_vect = best_pipe.named_steps['pre'].named_transformers_['tf_word']\n        vec = tf_word_vect.transform([text_clean]).toarray()[0]\n        if vec.sum() == 0:\n            return \"Kh\u00f4ng c\u00f3 token TF-IDF n\u00e0o kh\u1edbp.\"\n        clf = best_pipe.named_steps['clf']\n        coef = clf.coef_[0][:len(tf_word_vect.get_feature_names_out())]\n        idxs = np.where(vec > 0)[0]\n        toks = tf_word_vect.get_feature_names_out()\n        token_scores = [(toks[i], vec[i]*coef[i]) for i in idxs]\n        token_scores_sorted = sorted(token_scores, key=lambda x: x[1], reverse=True)\n        top = token_scores_sorted[:top_k]\n        return \", \".join([f\"{t} ({s:.3f})\" for t,s in top])\n    except Exception as e:\n        return f\"L\u1ed7i explain: {e}\"\n\ndef predict_and_explain(caption, comment):\n    text = (caption or \"\") + \" \" + (comment or \"\")\n    if text.strip()==\"\":\n        return \"\u26a0\ufe0f Vui l\u00f2ng nh\u1eadp caption ho\u1eb7c comment!\", None, None\n    row = make_features_row_from_caption_comment(caption, comment)\n    prob = best_pipe.predict_proba(row)[:,1][0] if hasattr(best_pipe, \"predict_proba\") else None\n    pred = best_pipe.predict(row)[0]\n    label = \"\ud83d\udea8 Spam\" if int(pred)==1 else \"\u2705 H\u1ee3p l\u1ec7\"\n    prob_str = f\"{prob*100:.2f}% kh\u1ea3 n\u0103ng l\u00e0 Spam\" if prob is not None else \"N/A\"\n    explain = explain_top_tokens(text)\n    return label, prob_str, explain\n\nwith Blocks() as demo:\n    Markdown(\"## \ud83d\udd0e Nh\u1eadn di\u1ec7n Spam (TF-IDF + \u0111\u1eb7c tr\u01b0ng m\u1edf r\u1ed9ng)\")\n    with Row():\n        caption_in = Textbox(label=\"Caption b\u00e0i vi\u1ebft\", placeholder=\"Nh\u1eadp caption...\")\n        comment_in = Textbox(label=\"B\u00ecnh lu\u1eadn\", placeholder=\"Nh\u1eadp comment...\")\n    with Row():\n        out_label = Textbox(label=\"K\u1ebft qu\u1ea3\")\n        out_prob = Textbox(label=\"X\u00e1c su\u1ea5t (Spam)\")\n    out_explain = Textbox(label=\"Top t\u1eeb/c\u1ee5m t\u1eeb \u0111\u00f3ng g\u00f3p\")\n    btn = Button(\"\ud83d\ude80 D\u1ef1 \u0111o\u00e1n & Gi\u1ea3i th\u00edch\")\n    btn.click(fn=predict_and_explain, inputs=[caption_in, comment_in], outputs=[out_label, out_prob, out_explain])\n\ndemo.launch(share=True)\n", "outputs": []}]}